{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ca60ae14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting twikit\n",
      "  Downloading twikit-2.3.3-py3-none-any.whl.metadata (4.2 kB)\n",
      "Collecting httpx[socks] (from twikit)\n",
      "  Using cached httpx-0.28.1-py3-none-any.whl.metadata (7.1 kB)\n",
      "Collecting filetype (from twikit)\n",
      "  Using cached filetype-1.2.0-py2.py3-none-any.whl.metadata (6.5 kB)\n",
      "Collecting beautifulsoup4 (from twikit)\n",
      "  Downloading beautifulsoup4-4.13.5-py3-none-any.whl.metadata (3.8 kB)\n",
      "Collecting pyotp (from twikit)\n",
      "  Downloading pyotp-2.9.0-py3-none-any.whl.metadata (9.8 kB)\n",
      "Collecting lxml (from twikit)\n",
      "  Downloading lxml-6.0.1-cp312-cp312-macosx_10_13_universal2.whl.metadata (3.8 kB)\n",
      "Collecting webvtt-py (from twikit)\n",
      "  Downloading webvtt_py-0.5.1-py3-none-any.whl.metadata (3.4 kB)\n",
      "Collecting m3u8 (from twikit)\n",
      "  Downloading m3u8-6.0.0-py3-none-any.whl.metadata (4.9 kB)\n",
      "Collecting Js2Py-3.13 (from twikit)\n",
      "  Downloading Js2Py_3.13-0.74.1-py3-none-any.whl.metadata (1.0 kB)\n",
      "Collecting soupsieve>1.2 (from beautifulsoup4->twikit)\n",
      "  Downloading soupsieve-2.8-py3-none-any.whl.metadata (4.6 kB)\n",
      "Collecting typing-extensions>=4.0.0 (from beautifulsoup4->twikit)\n",
      "  Using cached typing_extensions-4.15.0-py3-none-any.whl.metadata (3.3 kB)\n",
      "Collecting anyio (from httpx[socks]->twikit)\n",
      "  Using cached anyio-4.10.0-py3-none-any.whl.metadata (4.0 kB)\n",
      "Collecting certifi (from httpx[socks]->twikit)\n",
      "  Using cached certifi-2025.8.3-py3-none-any.whl.metadata (2.4 kB)\n",
      "Collecting httpcore==1.* (from httpx[socks]->twikit)\n",
      "  Using cached httpcore-1.0.9-py3-none-any.whl.metadata (21 kB)\n",
      "Collecting idna (from httpx[socks]->twikit)\n",
      "  Using cached idna-3.10-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting socksio==1.* (from httpx[socks]->twikit)\n",
      "  Downloading socksio-1.0.0-py3-none-any.whl.metadata (6.1 kB)\n",
      "Collecting h11>=0.16 (from httpcore==1.*->httpx[socks]->twikit)\n",
      "  Using cached h11-0.16.0-py3-none-any.whl.metadata (8.3 kB)\n",
      "Collecting tzlocal>=1.2 (from Js2Py-3.13->twikit)\n",
      "  Using cached tzlocal-5.3.1-py3-none-any.whl.metadata (7.6 kB)\n",
      "Requirement already satisfied: six>=1.10 in ./.venv/lib/python3.12/site-packages (from Js2Py-3.13->twikit) (1.17.0)\n",
      "Collecting pyjsparser>=2.5.1 (from Js2Py-3.13->twikit)\n",
      "  Downloading pyjsparser-2.7.1.tar.gz (24 kB)\n",
      "  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting sniffio>=1.1 (from anyio->httpx[socks]->twikit)\n",
      "  Using cached sniffio-1.3.1-py3-none-any.whl.metadata (3.9 kB)\n",
      "Downloading twikit-2.3.3-py3-none-any.whl (82 kB)\n",
      "Downloading beautifulsoup4-4.13.5-py3-none-any.whl (105 kB)\n",
      "Using cached filetype-1.2.0-py2.py3-none-any.whl (19 kB)\n",
      "Using cached httpcore-1.0.9-py3-none-any.whl (78 kB)\n",
      "Downloading socksio-1.0.0-py3-none-any.whl (12 kB)\n",
      "Downloading Js2Py_3.13-0.74.1-py3-none-any.whl (611 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m611.8/611.8 kB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m-:--:--\u001b[0m\n",
      "\u001b[?25hDownloading lxml-6.0.1-cp312-cp312-macosx_10_13_universal2.whl (8.4 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.4/8.4 MB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading m3u8-6.0.0-py3-none-any.whl (24 kB)\n",
      "Downloading pyotp-2.9.0-py3-none-any.whl (13 kB)\n",
      "Downloading webvtt_py-0.5.1-py3-none-any.whl (19 kB)\n",
      "Downloading soupsieve-2.8-py3-none-any.whl (36 kB)\n",
      "Using cached typing_extensions-4.15.0-py3-none-any.whl (44 kB)\n",
      "Using cached tzlocal-5.3.1-py3-none-any.whl (18 kB)\n",
      "Using cached anyio-4.10.0-py3-none-any.whl (107 kB)\n",
      "Using cached idna-3.10-py3-none-any.whl (70 kB)\n",
      "Using cached certifi-2025.8.3-py3-none-any.whl (161 kB)\n",
      "Using cached httpx-0.28.1-py3-none-any.whl (73 kB)\n",
      "Using cached h11-0.16.0-py3-none-any.whl (37 kB)\n",
      "Using cached sniffio-1.3.1-py3-none-any.whl (10 kB)\n",
      "Building wheels for collected packages: pyjsparser\n",
      "  Building wheel for pyjsparser (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for pyjsparser: filename=pyjsparser-2.7.1-py3-none-any.whl size=26010 sha256=3212d76b57fd6a6f443613263fdf4903e093b21d3fd7491a3837daa363aaf1f4\n",
      "  Stored in directory: /Users/tiarasabrina/Library/Caches/pip/wheels/14/32/1d/9ef7b582e358446aeef4b9052aa89ef4dffa1688c1aae8aa13\n",
      "Successfully built pyjsparser\n",
      "Installing collected packages: pyjsparser, filetype, webvtt-py, tzlocal, typing-extensions, soupsieve, socksio, sniffio, pyotp, m3u8, lxml, idna, h11, certifi, Js2Py-3.13, httpcore, beautifulsoup4, anyio, httpx, twikit\n",
      "Successfully installed Js2Py-3.13-0.74.1 anyio-4.10.0 beautifulsoup4-4.13.5 certifi-2025.8.3 filetype-1.2.0 h11-0.16.0 httpcore-1.0.9 httpx-0.28.1 idna-3.10 lxml-6.0.1 m3u8-6.0.0 pyjsparser-2.7.1 pyotp-2.9.0 sniffio-1.3.1 socksio-1.0.0 soupsieve-2.8 twikit-2.3.3 typing-extensions-4.15.0 tzlocal-5.3.1 webvtt-py-0.5.1\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install twikit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f72f9f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Login OK & cookies saved.\n"
     ]
    }
   ],
   "source": [
    "import asyncio\n",
    "from twikit import Client\n",
    "\n",
    "USERNAME = 'username_twitter'\n",
    "EMAIL = 'email_twitter'\n",
    "PASSWORD = 'password_twitter'\n",
    "\n",
    "client = Client(\"en-US\")\n",
    "\n",
    "async def main():\n",
    "    await client.login(\n",
    "        auth_info_1=USERNAME,\n",
    "        auth_info_2=EMAIL,\n",
    "        password=PASSWORD,\n",
    "        cookies_file=\"cookies.json\"\n",
    "    )\n",
    "    print(\"Login OK & cookies saved.\")\n",
    "\n",
    "await main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "685c5d81",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Saved 500 rows (cap reached) → /Users/tiarasabrina/Documents/PROJECT/scrape_twitter/tweets_python_paged.csv\n"
     ]
    }
   ],
   "source": [
    "import csv, os, json, asyncio\n",
    "from datetime import datetime, timezone\n",
    "from twikit import Client, Forbidden, TooManyRequests, NotFound\n",
    "\n",
    "CSV_PATH = \"tweets_python.csv\"\n",
    "COOKIES  = \"cookies.json\"\n",
    "\n",
    "client = Client(\"en-US\")\n",
    "\n",
    "def _tweet_url(t):\n",
    "    uname = getattr(t.user, \"screen_name\", None) or getattr(t.user, \"username\", None)\n",
    "    tid   = getattr(t, \"id\", None) or getattr(t, \"rest_id\", None)\n",
    "    return f\"https://x.com/{uname}/status/{tid}\" if (uname and tid) else \"\"\n",
    "\n",
    "def _clean_text(s):\n",
    "    return (s or \"\").replace(\"\\n\", \" \").replace(\"\\r\", \" \").strip()\n",
    "\n",
    "async def save_search_paged(query=\"python\", product=\"Latest\", per_page=50, max_rows=500, out_path=\"tweets_python_paged.csv\"):\n",
    "    client.load_cookies(COOKIES)\n",
    "\n",
    "    try:\n",
    "        page = await client.search_tweet(query, product, count=per_page)\n",
    "    except NotFound:\n",
    "        if product.lower() == \"latest\":\n",
    "            print(\"'Latest' 404 → fallback ke 'Top'\")\n",
    "            page = await client.search_tweet(query, \"Top\", count=per_page)\n",
    "        else:\n",
    "            raise\n",
    "\n",
    "    written = 0\n",
    "    with open(out_path, \"w\", newline=\"\", encoding=\"utf-8\") as f:\n",
    "        w = csv.writer(f)\n",
    "        w.writerow([\"name\", \"username\", \"text\", \"created_at_utc\", \"url\"])\n",
    "\n",
    "        while True:\n",
    "            batch = list(page)\n",
    "            if not batch:\n",
    "                break\n",
    "\n",
    "            for t in batch:\n",
    "                uname = getattr(t.user, \"screen_name\", None) or getattr(t.user, \"username\", None)\n",
    "                w.writerow([\n",
    "                    _clean_text(t.user.name),\n",
    "                    uname or \"\",\n",
    "                    _clean_text(t.text),\n",
    "                    getattr(t, \"created_at\", \"\"),\n",
    "                    _tweet_url(t),\n",
    "                ])\n",
    "                written += 1\n",
    "                if written >= max_rows:\n",
    "                    print(f\"Saved {written} rows (cap reached) → {os.path.abspath(out_path)}\")\n",
    "                    return\n",
    "\n",
    "            # coba next page\n",
    "            try:\n",
    "                page = await page.next()\n",
    "            except TooManyRequests:\n",
    "                print(\"Rate limit. Tidur 15 detik…\")\n",
    "                await asyncio.sleep(15)\n",
    "                try:\n",
    "                    page = await page.next()\n",
    "                except Exception:\n",
    "                    break\n",
    "            except Exception:\n",
    "                break\n",
    "\n",
    "            await asyncio.sleep(2)  # jeda sopan\n",
    "\n",
    "    print(f\"Saved {written} rows → {os.path.abspath(out_path)}\")\n",
    "\n",
    "query = \"<isi sama kata kunci>\"\n",
    "await save_search_paged(query=query, product=\"Latest\", per_page=50, max_rows=500)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
